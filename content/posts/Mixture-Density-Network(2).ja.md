---
title: Mixture Density Network (2)
tags:
  - Machine Learning
  - Inversion
  - Deep Learning
categories:
  - 学習ノート
mathjax: true
abbrlink: 8bb8e4ec
slug: mixture-density-network-2
copyright: true
date: 2022-02-10 13:23:00
lang: ja
---
長すぎるので、パートに分けています。
<!-- more -->

# 数学の復習

## 最尤法とは

尤度とは、ある分布に適合する可能性のことです。

例えば、公平なコインを10回投げて、6回表が出る確率は？この確率が「確率」です - ある事象が発生する可能性。

一方、尤度は、ある結果が与えられたとき、それがある分布から来た可能性を求めることです。

例えば、コインを10回投げて6回表が出た場合、そのコインが公平である尤度は？

別の例：統計によると、関数解析の期末試験の点数は$N(80,4^2)$に従うとします。小黄さんが90点を取る確率は？これは確率を計算しています。

別の例：小黄さんが90点を取った場合、試験が$N(80,4^2)$に従う確率は？これが尤度です。

これを使って、尤度を最大化するパラメータを選択します。

では、尤度はどのように計算するのでしょうか？

## 尤度の求め方

これは実際には最尤推定です。

2年生の時にこの概念を学んだことに気づきましたが、今では完全に忘れてしまいました。

復習しましょう。

より簡単な問題でこの概念を理解しましょう。最も単純な分布は二項分布です。コイン投げを例に使います。

コイン投げを例に使う場合、暗黙の要件があります：各事象は独立でなければなりません。尤度の概念を使用するには、各事象が独立である必要があります。

公平なコインの場合、表が出る確率は0.5です - これが確率です。

一度投げて表が出たコインの場合、次に表が出る確率は（つまり、この二項分布のパラメータを決定する）？これが尤度です。

しかし明らかに、この例から尤度を計算することはできません。サンプルが小さすぎるからです。直感的には、確率を頻度で置き換えるのに十分なサンプルが必要です。より学術的には、これが大数の法則と中心極限定理です。

### 大数の法則と中心極限定理

これは数理統計学の最も基本的な概念です。復習するなら、徹底的に復習しましょう。

**例：** 表と裏が同じ確率の公平なコインを投げます。表の頻度を$v_n=S_n/n$とし、$S_n$は表の回数、nは総投げ回数とします。投げ続けると、頻度列${v_n}$に2つの現象が観察されます：

1. 頻度${v_n}$と確率pの絶対偏差$|v_n-p|$はnが増加するにつれて減少する傾向がありますが、0に収束するとは言えません。
2. 頻度のランダム性により、絶対偏差$|v_n-p|$は変動します。大きな偏差を排除することはできませんが、nが増加するにつれて、大きな偏差の可能性は小さくなります。**これは極限の新しい概念です。**

定義：

任意の$\varepsilon$に対して、
$$
\lim_{n\to\infty}P(|\xi_n-\xi|\ge\varepsilon)=0
$$
が成り立つとき、確率変数列{$\xi_n,n\in N$}は確率変数$\xi$に確率収束するといい、$\xi_n\to^{P}\xi$と表記します。

確率収束の意味：$\xi_n$と$\xi$の絶対偏差が任意の与えられた量以上である可能性は、nが増加するにつれて小さくなります。逆に、$|\xi_n-\xi|$が任意の与えられた量より小さい可能性は、nが増加するにつれて1に近づきます。

書き換えると：
$$
\lim_{n\to\infty}P(|\xi_n-\xi|\leq\varepsilon)=1
$$
例えば、列$v_n$はますますある数に近づいていきます。

これは実際には大数の法則と呼ばれます。

定理：

n回の独立な繰り返し実験で、事象Aが$k_n$回発生し、$P(A)=p$とします。任意の$\varepsilon>0$に対して：
$$
\lim_{n\to\infty}P(|\frac{k_n}{n}-p|<\varepsilon)=0
$$
これはベルヌーイの大数の法則で、チェビシェフの不等式を使って証明できます。

ベルヌーイの大数の法則は二項分布の特殊なケースです。

他にも：

| チェビシェフの法則 | 分散が有界な独立な$X_{1}, X_{2}, \cdots$ | 標本平均は期待平均に収束 |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ベルヌーイの法則 | $n_{A} \sim B(n, p)$ | $\frac{n_{A}}{n}\stackrel{P}{\longrightarrow}p$ |
| ヒンチンの法則 | 期待値$\mu$を持つ独立同分布の$X_{1}, X_{2}, \cdots$ | 標本平均は$\mu$に収束 |

チェビシェフの法則が最も一般的で、同一分布を必要としません。ヒンチンの法則は分布が同一の場合の特殊ケースです。ベルヌーイの法則は分布が二項分布の場合です。大数の法則の本質：標本平均は母平均に等しい。

確率論において、**中心極限定理**は独立同分布の確率変数列の平均を記述します。

具体的には、大数の法則は独立同分布の確率変数に対して、n→∞のとき、平均はほぼ確実に期待値に収束すると述べています。中心極限定理は、平均と期待値の差が1/nでスケーリングされた正規分布N(0,σ²)に近似的に従うと述べています。ここでσ²は元の確率変数の分散です。

**同一分布の中心極限定理：** X1, X2, …, Xnが同じ分布で互いに独立で、数学的期待値と分散を持つとします。すると任意のxに対して、標準化された和は標準正規分布に収束します。

## では実際にどう計算するのか？

コインを10回投げてみましょう。尤度関数は通常*L*（Likelihoodの略）と表記されます。「6回表、4回裏」を観察した場合、パラメータθの異なる値に対する尤度関数は：
$$
L(\theta;6H4T)=C^6_{10}*\theta^6*(1-\theta)^4
$$
この式のグラフを以下に示します。グラフから：パラメータθが0.6のとき、尤度関数は最大化されます。他のパラメータ値では、「6回表、4回裏」の確率は相対的に小さくなります。この賭けでは、次は表が出ると予想します。なぜなら観察に基づくと、このコインは0.6の確率で表が出る可能性が高いからです。

!["6H4T"尤度関数](https://raw.githubusercontent.com/lifeodyssey/Figurebed/master/image/gamewin202202131440250.png)

より一般的なシナリオに一般化すると、尤度関数の一般形式は各サンプルが発生する確率の積として表現できます。

![image-20220213144122733](https://raw.githubusercontent.com/lifeodyssey/Figurebed/master/image/gamewin202202131441773.png)

これには最尤推定も含まれますが、この論文の損失は最尤法のみを使用するので、今はスキップします。

参考文献：
lulaoshi.info/machine-learning/linear-model/maximum-likelihood-estimation
https://www.cnblogs.com/BlairGrowing/p/14877125.html

# コードを見る

では、論文のソースコードを見てみましょう。

長すぎる...しかもTensorFlowで書かれている...自分で書き直さないと...

MDNクラスは多出力、完全（対称）共分散を処理し、パラメータには以下が含まれます：
- n_mix: 混合数（デフォルト=5）
- hidden: 層と隠れユニット（デフォルト=[100]*5）
- lr: 学習率（デフォルト=1e-3）
- l2: L2正則化（デフォルト=1e-3）
- n_iter: 訓練イテレーション（デフォルト=1e4）
- batch: ミニバッチサイズ（デフォルト=128）

MDNクラスの主要メソッド：

1. **`__init__`**: すべてのパラメータを初期化するコンストラクタ
2. **`_predict_chunk`**: データのサブセットに対する推定を生成
3. **`predict`**: _predict_chunkをラップするメイン予測インターフェース
4. **`extract_predictions`**: 係数からモデル予測を抽出
5. **`fit`**: データ前処理とモデル訓練を処理する訓練メソッド
6. **`build`**: ニューラルネットワークアーキテクチャを構築
7. **`loss`**: 尤度を使用して損失を計算

